# LlamaIndex Using Local Model

A lot of the LlamaIndex articles out there use OpenAI, which was starting to burn a hole in my pocket in token costs.

Here's a code sample of how to use a local Ollama model instead, it's just [the tute from LlamaIndex's site](https://docs.llamaindex.ai/en/stable/getting_started/starter_example_local/).
